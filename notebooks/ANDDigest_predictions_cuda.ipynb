{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**This notebook was created as part of the AI NER module of the new version of ANDDigest system.**\n",
        "\n",
        "The module is aimed for the prediction of the correspondence of the names of biological entities to their types, based on the context in which they are mentioned, using the following fine-tuned models: \n",
        "\n",
        "```\n",
        "Timofey/PubMedBERT_Cell_Components_Context_Classifier\n",
        "Timofey/PubMedBERT_Genes_Proteins_Context_Classifier\n",
        "Timofey/PubMedBERT_Drugs_Metabolites_Context_Classifier\n",
        "Timofey/PubMedBERT_Diseases_Side_Effects_Context_Classifier\n",
        "Timofey/PubMedBERT_Pathways_Context_Classifier\n",
        "```\n",
        "In order to gain access to the fine-tuned models you need to perform the next steps:\n",
        "1. Create a free [huggingface](https://huggingface.co) account\n",
        "2. Generate your User Access Token, which can be done by following this [guide](https://huggingface.co/docs/hub/security-tokens).\n",
        "3. Agree to share your contact information (username and email, used for registration in step 1), with the developers of these models.\n",
        "4. Paste your generated token into the corresponding field when the 5th cell of this notebook is executed.\n",
        "\n",
        "**Input/Output Formats description:**\n",
        "\n",
        "> **Input data:**<br>\n",
        "The input data has a format similar to the datasets, used in the fine-tuning of these models. It is a file with a list of texts of pubmed abstracts, in which each line consists of two tab-separated parts:<br>\n",
        "<br>\n",
        "1. Abstract identification number (PMID)<br>\n",
        "_Please note that the regular expression used by the program is designed for numeric pubmed identifiers, which consist only of numbers. In case of a different format of identification numbers, it is necessary to make adjustments to the regular expression template located in the eighth cell of the notebook, in the regexp variable_\n",
        "2. Abstract text, where the analyzed name is replaced by the **\\<andsystem\\-candidate\\>** tag\n",
        "**Example:**\n",
        "```\n",
        "30342689   Early detection of Parkinson's disease through patient questionnaire and predictive modelling. Early detection of Parkinson's disease (PD) is important which can enable early initiation of therapeutic interventions and management strategies. However, methods for early detection still remain an unmet clinical need in <andsystem-candidate>. In this study, we use the Patient Questionnaire (PQ) portion from the widely used Movement Disorder Society-Unified Parkinson's Disease Rating Scale (MDS-UPDRS) to develop prediction models that can classify early PD from healthy normal using machine learning techniques that are becoming popular in biomedicine: logistic regression, random forests, boosted trees and support vector machine (SVM). We carried out both subject-wise and record-wise validation for evaluating the machine learning techniques.\n",
        "...\n",
        "```\n",
        "\n",
        "> **Output data:**<br>\n",
        "The output is a list of numeric values, where each line corresponds to a same line number of the input file. By default, all results are saved into the file, located in the output directory. The output file is presented by a list, containing the three columns separated by several space characters and has the following format:<br>\n",
        "1. Abstract identification number, corresponding to the input data\n",
        "2. Probability value that the name of the analyzed object doesn\\'t match the context typical for the object type that the used model is configured for\n",
        "3. Probability value that the name of the object matches the object type being checked by the model\n",
        "**Example:**\n",
        "```\n",
        "30342689   2.9851287308702013e-06   0.9999970197677612\n",
        "```\n",
        "\n",
        "**Data preprocessing, performed by the program:**\n",
        "\n",
        "It should be noted that the maximum length of input data for the BERT models is limited to 512 tokens (pseudo-words, presented in the transformer's dictionary). Therefore, the notebook performs check of the tokenized data, supplied to the input of the model, for its compliance with the maximum length. If the value is exceeded, the program splits an abstract into the separated sentences using the **_sent_tokenize_** function from the [Natural Language Toolkit Package](https://www.nltk.org), and shortens it by cutting off one sentence from the end of the abstract, after which it joins the sentences into an abstract again and repeats the tokenization process with the check for exceeding the maximum value by the tokens number, until the length becomes <= 512. In case when the tag of the checked object (\\<andsystem\\-candidate\\>) appear in the last sentence, the notebook cuts off a one sentence from the beginning instead."
      ],
      "metadata": {
        "id": "J0fSk_slQTvL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EaUJrNvu2cKK"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create folder where prediction results will be stored\n",
        "!mkdir '/content/output/'"
      ],
      "metadata": {
        "id": "JpGw1oEJwVQS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTZ3XmIEv396"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEVylCBwwZXe"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download( 'punkt' )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2uL3sevQHDKJ"
      },
      "outputs": [],
      "source": [
        "# Please Note, you need to enter your user token here, before the running of next cells.\n",
        "# An alternative way is to provide your token as use_auth_token parameter in the from_pretrained class.\n",
        "# For more information see https://huggingface.co/docs/transformers/main_classes/model\n",
        "\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_eMRXdrwjMF"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "import torch\n",
        "import re\n",
        "import sys\n",
        "\n",
        "from ipywidgets import IntProgress\n",
        "from IPython.display import display\n",
        "import time\n",
        "\n",
        "# Specify the used model here.\n",
        "model_dir = 'Timofey/PubMedBERT_Cell_Components_Context_Classifier'\n",
        "\n",
        "# Path to the input file:\n",
        "input_dir = '/content/pubmed_corpus.BERT_input.components.short.txt'\n",
        "\n",
        "# Path to the output file:\n",
        "output_file = open( '/content/output/pubmed_corpus.BERT_output.components.short.txt', 'w' )\n",
        "\n",
        "# The number of examples (input file lines) can be specified here. This value is optional and used only for the visualization of the progress bar:\n",
        "max_count = 146526\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained( model_dir ).to( 'cuda' )\n",
        "tokenizer = BertTokenizer.from_pretrained( model_dir )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OerCwaLcxHbS"
      },
      "outputs": [],
      "source": [
        "f = IntProgress( min = 0, max = max_count ) # instantiate the bar\n",
        "display(f) # display the bar\n",
        "\n",
        "regexp = r'^\\d+[\\t|\\s]+'\n",
        "regexp_tag = r'\\<andsystem\\-candidate\\>'\n",
        "\n",
        "counter = 0\n",
        "abstract = ''\n",
        "pmid = ''\n",
        "\n",
        "# output_file.write( \"PMID FALSE TRUE\\n\" )\n",
        "input_file = open( input_dir, 'rb' )\n",
        "for line in input_file:\n",
        "    line = line.decode( errors = 'replace' )\n",
        "    lp = re.findall( regexp, line )[ 0 ]\n",
        "    pmid = lp.rstrip()\n",
        "\n",
        "    line = line.replace( lp, '' )\n",
        "    abstract = line.rstrip()\n",
        "\n",
        "    input_seq = tokenizer.encode( abstract, add_special_tokens = True )\n",
        "\n",
        "    if ( len( input_seq ) < 512 ):\n",
        "        inputs = tokenizer( abstract, return_tensors = \"pt\" ).to( 'cuda' )\n",
        "        labels = torch.tensor( [ 1 ] ).unsqueeze( 0 ).cuda()\n",
        "        outputs = model( **inputs, labels = labels )\n",
        "\n",
        "        predicition = outputs.logits.softmax( dim = -1 ).tolist()\n",
        "    else:\n",
        "        while len( input_seq ) >= 512:\n",
        "            abstract_split = []\n",
        "            abstract_split = nltk.sent_tokenize( abstract )\n",
        "            lp2 = len( re.findall( regexp_tag, abstract_split[ -1 ] ) )\n",
        "            \n",
        "            if ( lp2 < 1 ):\n",
        "                abstract = abstract.replace( abstract_split[ -1 ], '' )\n",
        "            else:\n",
        "                abstract = abstract.replace( abstract_split[ 0 ], '' )\n",
        "            \n",
        "            input_seq = tokenizer.encode( abstract, add_special_tokens = True )\n",
        "                    \n",
        "        inputs = tokenizer( abstract, return_tensors = \"pt\" ).to( 'cuda' )\n",
        "        labels = torch.tensor( [ 1 ] ).unsqueeze( 0 ).cuda()  # Batch size 1\n",
        "        outputs = model( **inputs, labels = labels )\n",
        "\n",
        "        predicition = outputs.logits.softmax( dim = -1 ).tolist()\n",
        "\n",
        "    print( str( pmid ), \" \", predicition[ 0 ][ 0 ], \" \", predicition[ 0 ][ 1 ], file = output_file )\n",
        "\n",
        "    counter += 1\n",
        "    f.value += 1 # signal to increment the progress bar\n",
        "\n",
        "counter = 0\n",
        "abstract = ''\n",
        "pmid = ''\n",
        "\n",
        "input_file.close()\n",
        "output_file.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print( 'Done' )"
      ],
      "metadata": {
        "id": "_hI2ZaIWpnjZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}